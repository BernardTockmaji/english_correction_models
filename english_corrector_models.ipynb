{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\correct-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#For Implementing Gramformer Solution\n",
    "from gramformer import Gramformer\n",
    "\n",
    "#for implementing Bert Solution\n",
    "from happytransformer import HappyTextToText, TTSettings\n",
    "\n",
    "#For Implementing LanguageTool Solution\n",
    "import language_tool_python\n",
    "\n",
    "#For GingerIt Solution\n",
    "from gingerit.gingerit import GingerIt\n",
    "\n",
    "#For Symspellpy\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell\n",
    "\n",
    "#For TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "#For Web Application Development\n",
    "import gradio as gr\n",
    "\n",
    "import pandas as pd\n",
    "import sys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./grammatical error detection/NLP Assignment/test_data.csv')\n",
    "df.head(20)\n",
    "test_df = df.head(40)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gramformer Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gramformer] Grammar error correct/highlight model loaded..\n"
     ]
    }
   ],
   "source": [
    "gf = Gramformer(models=1, use_gpu=False) #1=corrector, 2=detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello my dear child.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf.correct('hello my dear childs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Function to build web application using gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(sentence):\n",
    "    res = gf.correct(sentence) \n",
    "    return res\n",
    "# app_inputs = gr.inputs.Textbox(lines=3, placeholder=\"Enter a grammatically incorrect sentence here...\")\n",
    "\n",
    "# interface = gr.Interface(fn=correct, \n",
    "#                         inputs=app_inputs,\n",
    "#                          outputs='text', \n",
    "#                         title='Hi there, I\\'m Gramformer')\n",
    "\n",
    "#interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Gramformer highlighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gh = Gramformer(models=3, use_gpu=False) #1=corrector, 2=detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gh.highlight(orig='to tha store',cor='to the store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Gramformer on Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gramformer_corrector(text):\n",
    "    res = gf.correct(text) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Calling Gramformer model for correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['corrected_sentence'] = test_df['input'].apply(lambda text: gramformer_corrector(text))\n",
    "# test_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert + Huggingface Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence has bad grammar.\n"
     ]
    }
   ],
   "source": [
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "args = TTSettings(num_beams=5, min_length=1)\n",
    "result = happy_tt.generate_text(\"grammar: This sentences has has bads grammar.\", args=args)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_corrector(text):\n",
    "    result = happy_tt.generate_text('grammar: ' + text, args=TTSettings(num_beams=1, min_length=1, max_length=100))\n",
    "    return result.text\n",
    "# app_hugginface_inputs = gr.inputs.Textbox(lines=3, placeholder=\"Enter a grammatically incorrect sentence here...\")\n",
    "\n",
    "# interface2 = gr.Interface(fn=huggingface_corrector, \n",
    "#                         inputs=app_hugginface_inputs,\n",
    "#                         outputs='text', \n",
    "#                         title='Hi there, I\\'m Huggingface')\n",
    "\n",
    "#interface2.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Bert + Hugging Face on Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_corrector(text):\n",
    "    result = happy_tt.generate_text('grammar: ' + text, args=TTSettings(num_beams=1, min_length=1, max_length=100))\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Calling Bert model for correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['corrected_sentence'] = test_df['input'].apply(lambda text: huggingface_corrector(text))\n",
    "# test_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Tool Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tool = language_tool_python.LanguageTool('en-US')  \n",
    "my_text = \"\"\"LanguageTool provides utility to check grammar and spelling errors. We just have to paste the text here and click the 'Check Text' button. Click the colored phrases for for information on potential errors. or we can use this text too see an some of the issues that LanguageTool can dedect. Whot do someone thinks of grammar checkers? Please not that they are not perfect. Style problems get a blue marker: It is 7 P.M. in the evening. The weather was nice on Monday, 22 November 2021\"\"\"   \n",
    "\n",
    "def english_text_corrector(tool, text):\n",
    "    \n",
    "    matches = tool.check(text)\n",
    "\n",
    "    #empty lists\n",
    "    Mistakes = [] \n",
    "    Corrections = []  \n",
    "    StartPositions = []  \n",
    "    EndPositions = []  \n",
    "\n",
    "    for rules in matches:\n",
    "        if len(rules.replacements) > 0:  \n",
    "            StartPositions.append(rules.offset)  \n",
    "            EndPositions.append(rules.errorLength + rules.offset)  \n",
    "            Mistakes.append(my_text[rules.offset : rules.errorLength + rules.offset])  \n",
    "            Corrections.append(rules.replacements[0]) \n",
    "\n",
    "    print(\"Mistakes made\")\n",
    "    print (Mistakes)\n",
    "    print (\"\\nRecommended Corrections\")\n",
    "    print(Corrections)\n",
    "    print (\"\\nMistake Starting character number\")\n",
    "    print(StartPositions)\n",
    "    print (\"\\nMistake EndPoint character number\")\n",
    "    print(EndPositions)\n",
    "\n",
    "    mistakes_number = len (Mistakes)\n",
    "\n",
    "    print( \"\\nNumber of mistakes made \" + str(mistakes_number))\n",
    "    #return mistakes_number\n",
    "\n",
    "#english_text_corrector(my_tool, my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Tool Model on Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "def language_tool_corrector(text:str):\n",
    "    correction = tool.correct(text)\n",
    "    return correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Calling language tool model for correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['corrected_sentence'] = test_df['input'].apply(lambda text: language_tool_corrector(text))\n",
    "# test_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GingerIt Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The smell of flowers brings back memories.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The smelt of fliwers bring back memories.'\n",
    "\n",
    "parser = GingerIt()\n",
    "parser.parse(text)['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying GingerIt Model on Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ginger_corrector(text: str):\n",
    "    \n",
    "    parser = GingerIt()\n",
    "    correction = parser.parse(text)['result']\n",
    "    return correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Calling GingerIt model for correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['corrected_sentence'] = test_df['input'].apply(lambda text: ginger_corrector(text))\n",
    "# test_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symspellpy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the love he had dated for much of the past who couldn't read in six grade and inspired him, 9, 0\n"
     ]
    }
   ],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
    ")\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\"\n",
    ")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "input_term = (\n",
    "    \"whereis th elove hehad dated forImuch of thepast who \"\n",
    "    \"couqdn'tread in sixtgrade and ins pired him\"\n",
    ")\n",
    "# max edit distance per lookup (per single word, not per whole input string)\n",
    "suggestions = sym_spell.lookup_compound(input_term, max_edit_distance=2)\n",
    "\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symspelly_corrector(text):\n",
    "    sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
    "    )\n",
    "    bigram_path = pkg_resources.resource_filename(\n",
    "        \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\"\n",
    "    )\n",
    "\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "\n",
    "    for suggestion in suggestions:\n",
    "        return suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['corrected_sentence'] = test_df['input'].apply(lambda text: symspelly_corrector(text))\n",
    "# test_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob_corrector(text):\n",
    "    correction = TextBlob(text)\n",
    "    return correction.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['ginger_corrected_sentence'] = test_df['input'].apply(lambda text: textblob_corrector(text))\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining GingerIt with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['ginger_corrected_sentence'] = test_df['input'].apply(lambda text: ginger_corrector(text))\n",
    "# test_df['combined_with_bert_corrected_sentence'] = test_df['ginger_corrected_sentence'].apply(lambda text: huggingface_corrector(text))\n",
    "# test_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Gingerit with Gramformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['ginger_corrected_sentence'] = test_df['input'].apply(lambda text: ginger_corrector(text))\n",
    "# test_df['combined_with_gramformer'] = test_df['ginger_corrected_sentence'].apply(lambda text: gramformer_corrector(text))\n",
    "# test_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('correct-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4374edbdce5cf0ebc5572306dfb5843c5403ff613b25869cdbf8c66a66663275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
